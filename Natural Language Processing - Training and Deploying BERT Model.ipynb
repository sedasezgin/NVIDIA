{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38bd30f",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "In an era where computers, smartphones, and other electronic devices increasingly need to interact with humans, Natural Language Processing or NLP has become an indispensable technique for teaching devices how to communicate in natural languages in human-like ways. In this module we will understand what Natural Language Processing or NLP means and how we will teach devices to understand constructs of language like paragraphs, sentences and words.\n",
    "\n",
    "In this notebook we will explore the ideas mentioned video section of this course. We will start with the definition of Language modeling and where it can be used. Next we will explore BERT, a powerful NLP algorithm that is being used in many popular services like Search, Grammar Correction, Voice agents and Productivity software. We learn what it requires to train these multi-million parameter models on state of the art NVIDIA Graphic Processing Units or GPUs. TensorCores in GPUs are responsible for accelerated Deep Learning training so we will take a quick look at how we can use them to reduce our training time from days to hours by using Mixed Precision training methodology. We will take a pre-trained model and adapt it to work for our NLP task - Question and Answering. And then we will deploy this trained BERT model as a SageMaker endpoint enabling anyone to send a Question-Answering NLP task to your service.\n",
    "\n",
    "This will be an exciting journey, where at the end of it, you would have trained and deployed a state of the art NLP Model which can be used in your personal projects like building Chatbots, Grammar correction, sentiment analysis or other language understanding tasks. But let's start with with basics - Language Modeling\n",
    "\n",
    "## Language modeling – the basics\n",
    "## What is language modeling?\n",
    "\"Language modeling is the task of assigning a probability to sentences in a language. […] Besides assigning a probability to each sequence of words, the language models also assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\" Source: Page 105, Neural Network Methods in Natural Language Processing, 2017.\n",
    "\n",
    "## Types of language models\n",
    "There are primarily two types of Language Models:\n",
    "\n",
    "Statistical Language Models: These models use traditional statistical techniques like N-grams, Hidden Markov Models (HMM), and certain linguistic rules to learn the probability distribution of words.\n",
    "\n",
    "Neural Language Models: They use different kinds of Neural Networks to model language, and have surpassed the statistical language models in their effectiveness.\n",
    "\n",
    "\"We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.\" Source: Recurrent neural network based language model, 2010.\n",
    "\n",
    "Given the superior performance of neural language models, we include in the container two popular state-of-the-art neural language models: BERT and Transformer-XL.\n",
    "\n",
    "## Why is language modeling important?\n",
    "Language modeling is fundamental in modern NLP applications. It enables machines to understand qualitative information, and enables people to communicate with machines in the natural languages that humans use to communicate with each other.\n",
    "\n",
    "Language modeling is used directly in a variety of industries, including tech, finance, healthcare, transportation, legal, military, government, and more -- actually, you probably have just interacted with a language model today, whether it be through Google search, engaging with a voice assistant, or using text autocomplete features.\n",
    "\n",
    "## How does language modeling work?\n",
    "The roots of modern language modeling can be traced back to 1948, when Claude Shannon published a paper titled \"A Mathematical Theory of Communication\", laying the foundation for information theory and language modeling. In the paper, Shannon detailed the use of a stochastic model called the Markov chain to create a statistical model for the sequences of letters in English text. The Markov models, along with n-gram, are still among the most popular statistical language models today.\n",
    "\n",
    "However, simple statistical language models have serious drawbacks in scalability and fluency because of its sparse representation of language. Overcoming the problem by representing language units (eg. words, characters) as a non-linear, distributed combination of weights in continuous space, neural language models can learn to approximate words without being misled by rare or unknown values.\n",
    "\n",
    "Therefore, as mentioned above, we introduce two popular state-of-the-art neural language models, BERT and Transformer-XL, in Tensorflow and PyTorch. More details can be found in the NVIDIA Deep Learning Examples Github Repository\n",
    "\n",
    "## What is BERT ?\n",
    "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. This model is based on the BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding paper. NVIDIA's implementation of BERT is an optimized version of the Hugging Face implementation, leveraging mixed precision arithmetic and Tensor Cores on Volta V100 and Ampere A100 GPUs for faster training times while maintaining target accuracy.\n",
    "\n",
    "## What can BERT do ?\n",
    "With research organizations globally having conversational AI as the immediate goal in mind, BERT has made major breakthroughs in the field of NLP. In the past, basic voice interfaces like phone tree algorithms—used when you call your mobile phone company, bank, or internet provider—are transactional and have limited language understanding.\n",
    "\n",
    "With transactional interfaces, the scope of the computer’s understanding is limited to a question at a time. This gives the computer a limited amount of required intelligence: only that related to the current action, a word or two or, further, possibly a single sentence. For more information, see What is Conversational AI?\n",
    "\n",
    "But when people converse in their usual conversations, they refer to words and context introduced earlier in the paragraph. Going beyond single sentences is where conversational AI comes in.\n",
    "\n",
    "Here’s an example of using BERT to understand a passage and answer the questions. This example is taken from NVIDIA's NGC Site\n",
    "\n",
    "Passage: NGC Containers are designed to enable a software platform centered around minimal OS requirements, Docker and driver installation on the server or workstation, and provisioning of all application and SDK software in the NGC containers through the NGC container registry. NGC manages a catalog of fully integrated and optimized deep learning framework containers that take full advantage of NVIDIA GPUs in both single GPU and multi-GPU configurations.\n",
    "\n",
    "Next we can ask our question: What configurations can NGC containers work with?\n",
    "\n",
    "Soon you will see that BERT model to take this paragraph and question as input can provide the answer. We should expect to see something like this\n",
    "\n",
    "Answer: 'single GPU and multi-GPU'\n",
    "\n",
    "As dicussed in the video section of this course, Question answering is one of the GLUE benchmark metrics. A breakthrough is the development of the Stanford Question Answering Dataset or SQuAD, as it is the key to a robust and consistent training and standardizing learning performance observations. For more information, see SQuAD: 100,000+ Questions for Machine Comprehension of Text. In 2018, BERT became a popular deep learning model as it peaked the GLUE (General Language Understanding Evaluation) score to 80.5% (a 7.7% point absolute improvement). Question For more information, see A multi-task benchmark and analysis platform for natural understanding.\n",
    "\n",
    "After the development of BERT at Google, it was not long before NVIDIA achieved a world record time using massive parallel processing by training BERT on many GPUs. With some additional work we can also generate the time required to answer this question with the BERT model and how many questions can we ask in a second. For example, NVIDIA recently published similar results for BERT and a few other models. They were able to run about 17K questions on an NVIDIA A100 GPU. If you are interested in performance , then you would love this page with some fascinating performance numbers on NVIDIA Deep Learning training and Inference. In the next few sections we will look at how GPUs can be used to accelerate BERT Training and Inference. Before going into that, let's take a quick look at the architecture of the BERT model.\n",
    "\n",
    "The architecture and components of this model will give us a good idea on why GPUs can be used to accelerate this model.\n",
    "\n",
    "## BERT Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a9583",
   "metadata": {},
   "source": [
    "BERT stands for Bidirectional Encoder Representation of Transformers. In this video section, We already discussed the architecture of this model. Here is a quick recap.\n",
    "\n",
    "BERT has three concepts embedded in its name - Bidirectional Encoder Representation of Transformers. Transformers are a neural network that learns the human language using self-attention, where a segment of words is compared against itself. The model learns how a given word's meaning is derived from every other word in the segment. For example, lets look a the statement shown in here taken from this https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409826e6",
   "metadata": {},
   "source": [
    "\"The animal did not cross the road because it was too tired\". In this statement, ‘it’ refers to the animal. If we changed the statement to:- \"The animal did not cross the road because it was too wide\"; the sentence is exactly the same but we removed “tired” and added “wide”. However, now it is more likely that ‘it’ refers to the road being wide. Through the self-attention mechanism, transformers learn to derive meaning and references for each word in a sentence.\n",
    "\n",
    "Second, bidirectional means that the Neural Network which treats the words in a sentence as time-series data are able to look at sentences from both directions. The older algorithms looked at words in a forward direction, trying to predict the next word, which ignores the context and information that the words are occurring later in the sentence provided. BERT uses self-attention to look at the entire input sentence at one time. Any relationships before or after the word are accounted for\n",
    "\n",
    "Finally, an encoder is a component of the encoder-decoder structure. You encode the input language into latent space, and then reverse the process with a decoder trained to re-create a different language. This is great for translation, as self-attention helps resolve the many differences that a language has in expressing the same ideas, such as the number of words or sentence structure.\n",
    "\n",
    "In BERT, you just take the encoding idea to create that latent representation of the input, but then use that as a feature input into several, fully connected layers to learn a particular language task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067b5b6",
   "metadata": {},
   "source": [
    "## How to use BERT ?\n",
    "There are two steps to making BERT learn to solve a problem for you. You first need to pretrain the transformer layers to be able to encode a given type of text into representations that contain the full underlying meaning. Then, you need to train the fully connected classifier structure to solve a particular problem, also known as fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6893c4",
   "metadata": {},
   "source": [
    "Pretraining is a massive endeavor that can require supercomputer levels of compute time and equivalent amounts of data. The open-source datasets most often used are the articles on Wikipedia, which constitute 2.5 billion words, and BooksCorpus, which provides 11,000 free-use texts. This culminates in a dataset of about 3.3 billion words.\n",
    "\n",
    "All that data can be fed into the network for the model to scan and extract the structure of language. At the end of this process, you should have a model that, in a sense, knows how to read. This model has a general understanding of the language, meaning of the words, context, and grammar.\n",
    "\n",
    "To have this model customized for a particular domain, such as finance, more domain-specific data needs to be added on the pretrained model. This allows the model to understand and be more sensitive to domain-specific jargon and terms.\n",
    "\n",
    "A word has several meanings, depending on the context. For example, a bear to a zoologist is an animal. To someone on Wall Street, it means a bad market. Adding specialized texts makes BERT customized to that domain. It’s a good idea to take the pretrained BERT offered on NGC and customize it by adding your domain-specific data.\n",
    "\n",
    "Fine-tuning is much more approachable, requiring significantly smaller datasets on the order of tens of thousands of labelled examples. BERT can be trained to do a wide range of language tasks.\n",
    "\n",
    "Despite the many different fine-tuning runs that you do to create specialized versions of BERT, they can all branch off the same base pretrained model. This makes the BERT approach often referred to as an example of transfer learning, when model weights trained for one problem are then used as a starting point for another. After fine-tuning, this BERT model took the ability to read and learned to solve a problem with it.\n",
    "\n",
    "## BERT Pretraining, Fine-tuning & Inference\n",
    "## Downloading Pre-Trained Model\n",
    "The first thing we want to download a pre-trained model that we can use for fine-tuning in the next step. For the BERT model, training works in two steps – Pre-training and Fine-Tuning. BERT is pretrained on unlabelled data. We hand it the downloaded text from all of Wikipedia and BookCorpus which includes 11,000 books across various genres and hope that at the end of pre-training, our model understands the structure of the English language as well as any human. While the data size is massive, the unlabelled nature is a huge benefit. The man-hours needed to generate any labels for such a massive amount of data would be at best impractical and at worst impossible. So how does BERT extract understanding from the raw text?\n",
    "\n",
    "BERT, like all neural networks, learns by minimizing a loss function for a sample task. The task BERT was designed for in the pre-training stage is twofold: Masked Language Modeling and Next Sentence Prediction. The Masked LM task will take an input segment and mask or withhold a random 15% of the words. The model then tries to predict the missing word and compares it with the ground truth word that was initially removed. Over millions of learning steps, this provides the network the ability to predict new words from the surrounding context. It learns both the word contexts but also grammatical structure. Next, Sentence Prediction presents the model with sentence pairs, with half of the time being 2 sentences that originally followed each other in the corpus. The other 50% are two random sentences, and the model is tasked to predict if the sentences go together or not. This is to provide the model with sentence-level comprehension for tasks like Question Answering. The key here is that the label that the model needs for training, that is, to nudge all of the weights in the correct direction, is generated automatically. We present the data to the model and wait as it runs it through over and over—billions of matrix operations to provide the learning-to-read experience.\n",
    "\n",
    "If we wanted to train the the BERT model from scratch on an 8 GPU P3DN instance, our results will be ready after about 11 days. Lucky for us, NVIDIA has done the hard work and made a trained checkpoiint available for us to download from NVIDIA GPU Cloud (NGC). Lets download and unzip the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded8d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/bert_pyt_ckpt_base_pretraining_amp_lamb/versions/19.09.0/zip -O bert_pyt_ckpt_base_pretraining_amp_lamb_19.09.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10360047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Extract the BERT model\n",
    "!unzip -u bert_pyt_ckpt_base_pretraining_amp_lamb_19.09.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9227f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.60.0\n",
      "  Downloading sagemaker-2.60.0.tar.gz (444 kB)\n",
      "     -------------------------------------- 444.4/444.4 kB 4.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from sagemaker==2.60.0) (22.1.0)\n",
      "Collecting boto3>=1.16.32\n",
      "  Downloading boto3-1.28.28-py3-none-any.whl (135 kB)\n",
      "     ---------------------------------------- 135.8/135.8 kB ? eta 0:00:00\n",
      "Collecting google-pasta\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from sagemaker==2.60.0) (1.22.4)\n",
      "Collecting protobuf>=3.1\n",
      "  Downloading protobuf-4.24.0-cp38-cp38-win_amd64.whl (430 kB)\n",
      "     ------------------------------------- 430.6/430.6 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from sagemaker==2.60.0) (5.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from sagemaker==2.60.0) (21.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from sagemaker==2.60.0) (1.5.1)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.1/82.1 kB ? eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.8/79.8 kB ? eta 0:00:00\n",
      "Collecting botocore<1.32.0,>=1.31.28\n",
      "  Downloading botocore-1.31.28-py3-none-any.whl (11.1 MB)\n",
      "     ---------------------------------------- 11.1/11.1 MB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from importlib-metadata>=1.4.0->sagemaker==2.60.0) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from packaging>=20.0->sagemaker==2.60.0) (3.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==2.60.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from pandas->sagemaker==2.60.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from pandas->sagemaker==2.60.0) (2022.6)\n",
      "Collecting multiprocess>=0.70.15\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.6/132.6 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting pox>=0.3.3\n",
      "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
      "Collecting dill>=0.3.7\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting ppft>=1.7.6.7\n",
      "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\seda.sezgin\\appdata\\roaming\\python\\python38\\site-packages (from botocore<1.32.0,>=1.31.28->boto3>=1.16.32->sagemaker==2.60.0) (1.26.12)\n",
      "Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.60.0-py2.py3-none-any.whl size=619377 sha256=64c1f6f9b5de909adfce0c3c9778095cec1a6f870a7a2f1f13be319e02402436\n",
      "  Stored in directory: c:\\users\\seda.sezgin\\appdata\\local\\pip\\cache\\wheels\\c5\\bf\\02\\42c444c186ffe02becb2176de38f373bddaed477aa4fc9f75f\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4022 sha256=a847487b9321805f205d177998889fe426453b272410e41431a326486a5aca24\n",
      "  Stored in directory: c:\\users\\seda.sezgin\\appdata\\local\\pip\\cache\\wheels\\69\\c6\\f2\\851abd3a16801ddd68bceaa97d515bca9a1f3f05cc84ab029a\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: smdebug_rulesconfig, protobuf, ppft, pox, jmespath, google-pasta, dill, protobuf3-to-dict, multiprocess, botocore, s3transfer, pathos, boto3, sagemaker\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed boto3-1.28.28 botocore-1.31.28 dill-0.3.7 google-pasta-0.2.0 jmespath-1.0.1 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 protobuf-4.24.0 protobuf3-to-dict-0.1.5 s3transfer-0.6.2 sagemaker-2.60.0 smdebug_rulesconfig-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update SageMaker SDK \n",
    "!pip install -U sagemaker==2.60.0\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel after installations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b50c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8932\\3111577285.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# importing libraries required for this course\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import os, tarfile, json\n",
    "import time, datetime\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "from sagemaker.utils import name_from_base\n",
    "from model_utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from model_utils.modeling import BertForQuestionAnswering, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from model_utils.tokenization import (BasicTokenizer, BertTokenizer, whitespace_tokenize)\n",
    "from types import SimpleNamespace\n",
    "from helper_funcs import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'NLP-course'\n",
    "\n",
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-dllogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c9d00",
   "metadata": {},
   "source": [
    "## Downloading Dataset for Fine-Tuning\n",
    "For the BERT model, training works in two steps – Pretraining and Fine-Tuning. We will talk about these steps in a bit more detail in the next section. As we mentioned earlier, BERT is pretrained on unlabeled data from all of Wikipedia and BookCorpus, which includes 11,000 books across various genres. This data set is publicly available, but due to its large size, it is impractical to download and run training from scratch. Instead, for this class, we will work on fine-tuning the BERT model with the SQUAD Dataset.\n",
    "\n",
    "SQUAD stands for Stanford Question Answering Dataset. This dataset is a collection of passages and a question. The answer to the question is in the passage, and our task is to predict it accurately. With the command shown in this cell, we can download the publicly available SQUAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba2b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the SQUAD dataset for fine-tuning\n",
    "!cd data/squad/ && bash squad_download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e856945",
   "metadata": {},
   "source": [
    "## Reviewing SQUAD Dataset¶\n",
    "Now let's take a quick look at some examples in this dataset. In this cell block, we are picking a random passage, a Question, and the Answer in the dataset. This passage is about the Southern California. After the passage, the question is - What is Southern California often abbreviated as? The answer from within the passage is - SoCal.\n",
    "\n",
    "Our goal is to train the BERT model to find out the answer to a given question from the passage from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074be8da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/squad/v2.0/dev-v2.0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\1538324129.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the v2.0 dev set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/squad/v2.0/dev-v2.0.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msquad_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/squad/v2.0/dev-v2.0.json'"
     ]
    }
   ],
   "source": [
    "# load the v2.0 dev set\n",
    "with open('data/squad/v2.0/dev-v2.0.json', 'r') as f:\n",
    "    squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a1d294",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'squad_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\489382327.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#ind = random.randint(0,34) #for a random paragraph and question, set this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquad_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Paragraph title: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'context'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'squad_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Looking at specific instances of the SQUAD dataset\n",
    "#ind = random.randint(0,34) #for a random paragraph and question, set this\n",
    "ind = 2\n",
    "sq = squad_data['data'][ind]\n",
    "print('Paragraph title: ',sq['title'], '\\n')\n",
    "print(sq['paragraphs'][0]['context'],'\\n')\n",
    "print('Question:', sq['paragraphs'][0]['qas'][0]['question'])\n",
    "print('Answer:', sq['paragraphs'][0]['qas'][0]['answers'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebc69d",
   "metadata": {},
   "source": [
    "## View BERT input\n",
    "BERT needs us to transform our text data into a numeric representation known as tokens. There are a variety of tokenizers available, we are going to use a tokenizer specially designed for BERT that we will instantiate with our vocabulary file. Let's take a look at our transformed question and context we will be supplying BERT for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7608214e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\3545502657.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Seeing the input words and the associated tokens from the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'context'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vocab'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mquery_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'qas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sq' is not defined"
     ]
    }
   ],
   "source": [
    "# Seeing the input words and the associated tokens from the vocabulary\n",
    "doc_tokens = sq['paragraphs'][0]['context'].split()\n",
    "tokenizer = BertTokenizer('vocab', do_lower_case=True, max_len=512)\n",
    "query_tokens = tokenizer.tokenize(sq['paragraphs'][0]['qas'][0]['question'])\n",
    "\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=384, \n",
    "                                    max_query_length=64)\n",
    "\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "print(vars(tokens_for_postprocessing)[\"tokens\"][0:9])\n",
    "print(vars(tensors_for_inference)[\"input_ids\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7e5d6",
   "metadata": {},
   "source": [
    "## Finetuning witth SQUAD Dataset\n",
    "We have downloaded the BERT Model from NGC and also know about Mixed Precision for speeding-up training and fine-tuning. Now, let us fine-tune the BERT model to do well on the Question & Answering task.\n",
    "\n",
    "We will run fine-tuning using run_squad.py. Let's look at the key inputs to this python file.\n",
    "\n",
    "out_dir - Specify the output directory for the results to be written to.\n",
    "Init_checkpoint - Specify the BERT model to be used as the starting model\n",
    "Num_train_epochs - Each epoch typically consists of one round of training on the data. Specify the number of training rounds\n",
    "Vocab_file - Specify the vocabulary used for tokenizing BERT input\n",
    "Config_file - Specify the configuration details of the BERT model\n",
    "Train_file - Specify the file with data for training the model\n",
    "Predict_file - Link to file that is used to test for unbiased prediction.\n",
    "do_train - specify if you want to train/fine-tune the model\n",
    "Train_batch_size - Batch size for the input\n",
    "Max_seq_length - The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded\n",
    "doc_stride - When splitting up a long document into chunks, how much stride to take between chunks\n",
    "seed - random seed for the initialization\n",
    "fp16 - Use 16-bit precision\n",
    "You can get more details of various other options by running python run_squad.py --help\n",
    "\n",
    "Now that we have learned about various options to fine-tune the BERT model let's start fine-tuning the model. As you run the model, there are verbose logs that tell you details about Training Epoch, Iteration, the current loss, and the learning rate. With the --fp16 flag, each epoch should take around 45 minutes on AWS EC2 g4dn.xl instance, which contains NVIDIA T4 GPU. If you run the fine-tuning with FP32, then it might take 3-5 hours on the same GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985d4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'run_squad.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning BERT with SQUAD dataset\n",
    "# If this script fails due to out of memory errors, then reduce the \"train_batch_size\" value to 4 or 2\n",
    "!python run_squad.py --bert_model bert_base_uncased \\\n",
    "            --output_dir './output' \\\n",
    "            --init_checkpoint bert_base.pt \\\n",
    "            --num_train_epochs 1 \\\n",
    "            --vocab_file './vocab' \\\n",
    "            --config_file  './bert_config.json' \\\n",
    "            --train_file './data/squad/v1.1/train-v1.1.json' \\\n",
    "            --predict_file './data/squad/v1.1/dev-v1.1.json' \\\n",
    "            --do_train \\\n",
    "            --train_batch_size 8 \\\n",
    "            --max_seq_length 512 \\\n",
    "            --doc_stride 128 \\\n",
    "            --seed 1 \\\n",
    "            --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed920e8",
   "metadata": {},
   "source": [
    "## Loading the fine-tuned model¶\n",
    "Now our model is trained and stored in the folder named output. Let us load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a9b57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\3821137505.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Loading the BERT model trained with Question-Answering task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# specify the vocabulary file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./vocab'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#Loading the BERT model trained with Question-Answering task\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# specify the vocabulary file\n",
    "vocab_file='./vocab'\n",
    "\n",
    "# set variables that limit the maximum length of the context, query, and answer\n",
    "max_seq_length, max_query_length, n_best_size, max_answer_length, null_score_diff_threshold = 384, 64, 1, 30, -11.0\n",
    "do_lower_case, can_give_negative_answer = True, True\n",
    "\n",
    "# initialize our tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case=True, max_len=512)\n",
    "\n",
    "# load a model configuration\n",
    "config = BertConfig.from_json_file('bert_config.json')\n",
    "\n",
    "# set up our model architecture\n",
    "model = BertForQuestionAnswering(config)\n",
    "\n",
    "# load our weights\n",
    "model.load_state_dict(torch.load('./output/model.pth', map_location='cpu')[\"model\"])\n",
    "\n",
    "# send out model to our device\n",
    "model.to(device)\n",
    "    \n",
    "# set our model to evaluation mode for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360f78f",
   "metadata": {},
   "source": [
    "## Testing the fine-tuned model\n",
    "Our trained model is now loaded. Let us test it on our local instance. In the next couple of cells, we are going to provide it with a context paragraph and a question. Feel free to edit and change the context paragraph and the question.\n",
    "\n",
    "After that we will pre-process the input data and feed it to our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2f2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the fine-tuned model.\n",
    "# Provide your custom context and question. \n",
    "\n",
    "context = \"\"\"\n",
    "NGC Containers are designed to enable a software platform centered around minimal OS requirements, \n",
    "Docker and driver installation on the server or workstation, and provisioning of all application and SDK software \n",
    "in the NGC containers through the NGC container registry. NGC manages a catalog of fully integrated and optimized \n",
    "deep learning framework containers that take full advantage of NVIDIA GPUs in both single GPU and \n",
    "multi-GPU configurations. \n",
    "\"\"\"\n",
    "question = \"What configurations can NGC containers work with?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb91886a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\1981305144.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdoc_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# tokenize our query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mquery_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# generate features to feed to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m feature = preprocess_tokenized_text(doc_tokens, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Pre-processing the input and feeding it into the BERT model\n",
    "\n",
    "# specify how many answers to return, here we are going to take the top answer only.\n",
    "n_best_size=1\n",
    "\n",
    "# preprocessing\n",
    "# split the context into tokens\n",
    "doc_tokens = context.split()\n",
    "# tokenize our query \n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "# generate features to feed to the model\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=max_seq_length, \n",
    "                                    max_query_length=max_query_length)\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "\n",
    "input_ids = torch.tensor(tensors_for_inference.input_ids, dtype=torch.long).unsqueeze(0)\n",
    "segment_ids = torch.tensor(tensors_for_inference.segment_ids, dtype=torch.long).unsqueeze(0)\n",
    "input_mask = torch.tensor(tensors_for_inference.input_mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "# load tensors to device\n",
    "input_ids = input_ids.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "\n",
    "# run inference\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "# post-processing\n",
    "start_logits = start_logits[0].detach().cpu().tolist()\n",
    "end_logits = end_logits[0].detach().cpu().tolist()\n",
    "# convert logits back to English\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         start_logits, end_logits, n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "\n",
    "# print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264b122",
   "metadata": {},
   "source": [
    "## Prepare to deploy as a SageMaker Endpoint.\n",
    "Now that you've gotten a chance to play with the model locally, let's deploy it to an endpoint! In order to deploy BERT to a sagemaker endpoint, we need to save the model as a tarball. Once we have saved our model we then upload to our S3 bucket where our Docker container can access it. We use transform_script.py to define how we load our model, handle our input data, perform inference, and pass our results back to the requester.\n",
    "\n",
    "Sagemaker has predefined functions for all of these operations aside from importing the model, however, for our specific case we are passing in multiple arrays as input (our question and our provided context). This means we need to specify custom functions for our input data and making predictions. These functions are named input_fn and predict_fn inside of transform_script.py. To learn more about how to deploy PyTorch models in sagemaker see the following https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908b77ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tarfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\48136901.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save the model as a tarball\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert.tar.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w:gz'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./output/model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# upload model data to S3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tarfile' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the model as a tarball\n",
    "with tarfile.open('bert.tar.gz', 'w:gz') as f:\n",
    "    f.add('./output/model.pth')\n",
    "    \n",
    "# upload model data to S3\n",
    "model_data = sagemaker_session.upload_data(path='bert.tar.gz',\n",
    "                                           bucket=bucket,\n",
    "                                           key_prefix =os.path.join(prefix, 'model'))\n",
    "torch_model = PyTorchModel(model_data=model_data,\n",
    "                           role=role,\n",
    "                          entry_point='transform_script.py',\n",
    "                          framework_version='1.5.0',\n",
    "                          py_version=\"py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad67b14",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "Now that we have defined our model we can deploy it to an endpoint. We will need to give our endpoint a name, determine how many instances we want to run our endpoint, and the instance types. Here we are deploying this model to a g4dn instance that utilizes a Nvidia T4 card for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a65ecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\1676487403.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Deploy endpoint, this part may take a bit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mendpoint_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'bert-endpoint-{datetime.datetime.fromtimestamp(time.time()).strftime(\"%c\").replace(\" \",\"-\").replace(\":\",\"-\")}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m bert_end = torch_model.deploy(instance_type='ml.g4dn.xlarge', initial_instance_count=1, \n\u001b[0;32m      4\u001b[0m                               endpoint_name=endpoint_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Deploy endpoint, this part may take a bit\n",
    "endpoint_name = f'bert-endpoint-{datetime.datetime.fromtimestamp(time.time()).strftime(\"%c\").replace(\" \",\"-\").replace(\":\",\"-\")}'\n",
    "bert_end = torch_model.deploy(instance_type='ml.g4dn.xlarge', initial_instance_count=1, \n",
    "                              endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec15edd",
   "metadata": {},
   "source": [
    "## Get Predictions\n",
    "For question answering, we pass in a context statement for the model to read and then we ask it a question. In this first case we are doing the pre-processing locally and then sending the prepped data to the model as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d17494b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Collect the context and question, pre-process the input strings and feed it to the deployed BERT model\n",
    "\n",
    "n_best_size=3\n",
    "doc_tokens = context.split()\n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=max_seq_length, \n",
    "                                    max_query_length=max_query_length)\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "\n",
    "input_ids = np.array(tensors_for_inference.input_ids, dtype=np.int64)\n",
    "segment_ids = np.array(tensors_for_inference.segment_ids, dtype=np.int64)\n",
    "input_mask = np.array(tensors_for_inference.input_mask, dtype=np.int64)   \n",
    "\n",
    "payload = np.concatenate([np.expand_dims(input_ids, axis=0), np.expand_dims(segment_ids, axis=0), np.expand_dims(input_mask, axis=0)])\n",
    "try:\n",
    "    response = bert_end.predict(payload.tobytes(), initial_args={'ContentType':'application/x-npy'}) \n",
    "except:\n",
    "    print('using invoke_endpoint directly')\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                           ContentType='application/x-npy',\n",
    "                                           Body=payload.tobytes())\n",
    "    response = eval(response['Body'].read().decode('utf-8'))\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         response[0], response[1], n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "\n",
    "# print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')\n",
    "#print(f'inference took: {round(time.time()-t,4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e99fb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runtime_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'runtime_client' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pass_in_data = {'context':context, 'question':question}\n",
    "response = runtime_client.invoke_endpoint(EndpointName=bert_end.endpoint,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=json.dumps(pass_in_data))\n",
    "response = eval(response['Body'].read().decode('utf-8'))\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         response[0], response[1], n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "#print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eac83a",
   "metadata": {},
   "source": [
    "## Clean-up endpoint\n",
    "Ensure you delete the endpoint if you are not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915136bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\4291384672.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Delete endpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbert_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_end' is not defined"
     ]
    }
   ],
   "source": [
    "#Delete endpoint\n",
    "bert_end.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504a71e",
   "metadata": {},
   "source": [
    "## Clean-up rest of the files we created if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3df9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove the BERT model that we downloaded\n",
    "!rm bert.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee9e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove the BERT model that we downloaded\n",
    "!rm bert.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c57d77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove the zip file\n",
    "!rm bert_pyt_ckpt_base_pretraining_amp_lamb_19.09.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d10c1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Remove the SQUAD trained BERT Model\n",
    "!rm -r ./output/*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360803c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove SQUAD files\n",
    "!rm -r ./data/squad/v1.1/*.*\n",
    "!rm -r ./data/squad/v2.0/*.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
